require("glue")               # Easier string formatting
library("here")               # Easy shortcut for the root RStudio project dir
library("tidyverse")          # All useful tidyverse packages
library("fs")                 # Handling file operations
library("RedditExtractoR")    # Reddit crawling package
# Define the relevant directories.
base_dir <- here()
subreddits_dir <- glue("{base_dir}/subreddits")
logs_dir <- glue("{base_dir}/logs")
# Read in the helper functions.
source(glue("{base_dir}/helper-functions.R"))
# Initialize relevant variables.
list_filename <- glue("{base_dir}/list.subreddits.txt")
crawling_log_filename <- glue("{logs_dir}/log.crawling-progress.tsv")
event_log_filename <- glue("{logs_dir}/log.events.txt")
set_event_log_name(event_log_filename)
# DAYS_UNCHANGED <- 1    # How many days with no new comments until we declare a thread inactive?
DAYS_UNCHANGED <- 3    # How many days with no new comments until we declare a thread inactive?
CRAWL_THREADS <- 1
CRAWL_COMMENTS <- 2
PREFIX_LENGTH <- 3
MIN_COMMENT_CRAWLING_BREAK <- 2
MAX_COMMENT_CRAWLING_BREAK <- 6
PRINT_LOG_MESSAGE <- FALSE
PRINT_LOG_MESSAGE_CRAWLING <- TRUE
### Read in subreddit list and crawling log ####################################
# Read in the list of subreddits to be crawled.
subreddit_list <- get_subreddit_list(list_filename)
# Make sure the crawling log exists and is read in.
crawling_log <- get_crawling_log(crawling_log_filename, subreddit_list)
base_dir <- here()
subreddits_dir <- glue("{base_dir}/subreddits")
logs_dir <- glue("{base_dir}/logs")
# Read in the helper functions.
source(glue("{base_dir}/helper-functions.R"))
# Initialize relevant variables.
list_filename <- glue("{base_dir}/list.subreddits.txt")
crawling_log_filename <- glue("{logs_dir}/log.crawling-progress.tsv")
event_log_filename <- glue("{logs_dir}/log.events.txt")
set_event_log_name(event_log_filename)
# DAYS_UNCHANGED <- 1    # How many days with no new comments until we declare a thread inactive?
DAYS_UNCHANGED <- 3    # How many days with no new comments until we declare a thread inactive?
CRAWL_THREADS <- 1
CRAWL_COMMENTS <- 2
PREFIX_LENGTH <- 3
MIN_COMMENT_CRAWLING_BREAK <- 2
MAX_COMMENT_CRAWLING_BREAK <- 6
PRINT_LOG_MESSAGE <- FALSE
PRINT_LOG_MESSAGE_CRAWLING <- TRUE
### Read in subreddit list and crawling log ####################################
# Read in the list of subreddits to be crawled.
subreddit_list <- get_subreddit_list(list_filename)
get_crawling_log(crawling_log_filename, subreddit_list)
get_crawling_log(crawling_log_filename, subreddit_list)
crawling_log <- get_crawling_log(crawling_log_filename, subreddit_list)
if (nrow(crawling_log) > 1) {
inactive_threads <- crawling_log %>%
# Do not consider the threads that are marked as 'done'.
filter(status != "done") %>%
# Extract the timestamps and comment counts for the earliest and latest crawls
# of a thread.
group_by(subreddit, thread_id) %>%
mutate(first_c_ts = min(crawling_timestamp), first_comments = min(comments),
last_c_ts = max(crawling_timestamp), last_comments = max(comments)) %>%
# Calculate the differences and remove the previous helper columns.
mutate(diff_c_ts = as.numeric(last_c_ts - first_c_ts),
diff_now_ts = as.numeric(now() - last_c_ts),
diff_comments = last_comments - first_comments) %>%
select(-first_c_ts, -last_c_ts, -first_comments, -last_comments) %>%
# For the most recent crawl, check whether there have been no new comments
# in the specified time period.
mutate(status = ifelse(crawling_timestamp == max(crawling_timestamp) &
diff_comments == 0 &
(diff_c_ts >= DAYS_UNCHANGED | diff_now_ts >= DAYS_UNCHANGED),
"inactive",
status)) %>%
# Extract the thread IDs for these threads newly classified as inactive.
ungroup() %>%
filter(status == "inactive") %>%
select(thread_id) %>%
distinct() %>%
pull()
} else {
inactive_threads <- c()
}
# Assign the 'inactive' label to all inactive thread cases in the crawling log.
crawling_log <- crawling_log %>%
mutate(status = ifelse(thread_id %in% inactive_threads, "inactive", status))
# Make sure only the most recent rows for threads that are 'done' are retained in
# the crawling log.
crawling_log <- crawling_log %>%
# What row has the most recent timestamp?
group_by(subreddit, thread_id) %>%
mutate(max_crawling_timestamp = max(crawling_timestamp)) %>%
# Flag all non-recent 'done' rows as deleteable.
mutate(delete_row = ifelse(status == "done" &
crawling_timestamp != max_crawling_timestamp,
TRUE,
FALSE)) %>%
# Delete those rows and remove the helper column.
filter(delete_row == FALSE) %>%
select(-delete_row)
# Save this updated crawling log.
crawling_log %>% write_tsv(crawling_log_filename)
### Crawl threads ##############################################################
# Check whether it's time to crawl threads.
if (1) {
# if (pick_object_to_crawl() == CRAWL_THREADS) {
log_event(glue("Crawling threads..."), message = PRINT_LOG_MESSAGE)
# Crawl new threads from each subreddit in the list.
for (subreddit in subreddit_list) {
# Sleep for X seconds.
# What period are we crawling? All threads posted in the last hour/day/week?
# current_period <- "hour"
# current_period <- "day"
current_period <- "week"
# current_period <- "month"
# current_period <- "year"
# current_period <- "all"
# Attempt to crawl this subreddit, process the results and save them.
tryCatch({
# Get all threads from this subreddit for this period and convert to a tibble.
log_event(glue("Requesting threads from '{subreddit}'..."), message = PRINT_LOG_MESSAGE_CRAWLING)
# message(subreddit)
new_threads <- find_thread_urls(subreddit = subreddit, period = current_period)
current_ts <- now()
# Define the URL prefix to remove.
url_prefix <- glue("https://www.reddit.com/r/{subreddit}/comments/")
# Clean up the crawled threads.
cleaned_threads <- new_threads %>%
# Convert to tibble format.
as_tibble() %>%
# Remove NA lines.
filter(!is.na(title)) %>%
filter(text != "") %>%
# Add a readable timestamp column.
mutate(timestamp = as_datetime(timestamp)) %>%
# Clean the title and post text.
mutate(title = clean_text(title),
text = clean_text(text)) %>%
# Extract the thread ID from the URL.
mutate(thread_id = substr(str_remove(url, url_prefix), 1, 7)) %>%
# Rearrange and remove columns.
select(thread_id, timestamp, subreddit, title, text, comments, url) %>%
# Sort by newest threads.
arrange(desc(timestamp))
# Read in the threads currently crawled from this subreddit.
thread_crawl_dir <- glue("{subreddits_dir}/{subreddit}")
old_threads <- get_subreddit_crawl(thread_crawl_dir, subreddit)
# Merge the newly crawled threads with the old crawl.
all_threads <- rbind(cleaned_threads, old_threads) %>%
# Make sure that, if there are duplicates, we keep the row with the
# highest comment count.
group_by(thread_id) %>%
mutate(max_comments = max(comments)) %>%
filter(comments == max_comments) %>%
# Remove the double quotes that for some reason keep getting inserted.
mutate(title = str_replace_all(title, "[\"]+", "\""),
text = str_replace_all(text, "[\"]+", "\"")) %>%
mutate(title = str_squish(title),
text = str_squish(text)) %>%
# Remove the helper column.
select(-max_comments) %>%
distinct() %>%
# Sort in reverse chronological order (newest threads first).
arrange(desc(timestamp))
# Save the updated version to file.
log_event(glue("Saving new threads from '{subreddit}' to disk..."), message = PRINT_LOG_MESSAGE)
thread_crawl_filename <- glue("{thread_crawl_dir}/threads.{subreddit}.tsv")
write_tsv(all_threads, thread_crawl_filename)
# Log this crawling event in the crawling log.
log_event(glue("Logging crawl of new threads from '{subreddit}'..."), message = PRINT_LOG_MESSAGE)
cleaned_threads %>%
# Extract the required columns.
select(subreddit, thread_id, comments) %>%
# Add the status and crawling timestamp.
mutate(status = "active", crawling_timestamp = current_ts) %>%
relocate(crawling_timestamp, .before="comments") %>%
# Append to the crawling log.
write_tsv(crawling_log_filename, append = TRUE)  # Append, don't overwrite!
# If anything goes wrong, catch the error.
}, error = function(msg) {
log_event(glue("Error crawling new threads from '{subreddit}'..."), message = PRINT_LOG_MESSAGE)
}) # Close the tryCatch statement
} # Closes subreddit loop
} # Closes thread crawling IF
### Crawl comments #############################################################
# Check whether it's time to crawl comments.
if (1) {
# if (pick_object_to_crawl() == CRAWL_COMMENTS) {
log_event(glue("Crawling comments for threads that have become inactive"), message = PRINT_LOG_MESSAGE)
# Hardcode a list of problematic thread IDs not to crawl comments for.
thread_id_blacklist <- c("1kk5nxn", "1kfo6eu", "1lg01ah", "1ltjm60")
# Make sure the crawling log exists and is read in.
crawling_log <- get_crawling_log(crawling_log_filename, subreddit_list)
# At the end of this, we will change the status for all successfully crawled
# threads to 'done'. For this, start collecting the thread IDs for each
# successfully crawled thread.
successfully_crawled_threads_list <- c()
# For each subreddit
for (current_subreddit in subreddit_list) {
# for (current_subreddit in c("tipofmyjoystick")) {
# Extract a list of all thread IDs for which to crawl comments (= threads
# whose comment count hasn't changed for a while).
inactive_thread_list <- crawling_log %>%
filter(status == "inactive" & subreddit == current_subreddit) %>%
select(thread_id) %>%
distinct() %>%
pull()
# Read in the current crawl for this subreddit and extract all the thread URLs
thread_crawl_dir <- glue("{subreddits_dir}/{current_subreddit}")
current_sr_urls <- get_subreddit_crawl(thread_crawl_dir, current_subreddit) %>%
select(thread_id, url)
# Filter out the active threads (we only crawl inactive threads, i.e., threads
# whose comment count hasn't changed for a while).
current_sr_urls <- current_sr_urls %>% filter(thread_id %in% inactive_thread_list)
# Randomize the list to make sure we don't always get stuck on the same comments.
current_sr_urls <- slice(current_sr_urls, sample(1:n()))
# Get a list of all threads for this subreddit.
current_sr_threads <- select(current_sr_urls, thread_id) %>% pull()
# Get the list of threads we've already crawled at least once.
search_pattern <- glue("{subreddits_dir}/{current_subreddit}")
crawled_thread_ids <- dir_ls(search_pattern, recurse = TRUE) %>%
as_tibble() %>%
mutate(path = as.character(value)) %>%
filter(str_detect(path, "comments.")) %>%
mutate(thread_id = str_replace(str_sub(str_replace(path, search_pattern, ""), PREFIX_LENGTH + 12), ".tsv", "")) %>%
select(thread_id) %>%
distinct() %>%
pull()
# Get the list of threads we have not crawled yet.
uncrawled_thread_ids <- setdiff(current_sr_threads, crawled_thread_ids)
# Get the list of threads that are still marked as inactive but actually have
# already been crawled (so where something went wrong flipping these from
# "inactive" to "done").
inactive_crawled_thread_ids <- setdiff(current_sr_threads, uncrawled_thread_ids)
# Log a status message.
N_all <- length(current_sr_threads)
N_all_inactive <- length(inactive_thread_list)
N_all_crawled <- length(crawled_thread_ids)
N_all_inactive_crawled <- length(inactive_crawled_thread_ids)
N_all_inactive_uncrawled <- length(uncrawled_thread_ids)
if (N_all_inactive > 0) {
log_event(glue("Subreddit '{current_subreddit}' has {round0(N_all_inactive)} inactive thread(s) of which {N_all_inactive_uncrawled} ({round1(N_all_inactive_uncrawled / N_all_inactive * 100)}%) are uncrawled..."), message = PRINT_LOG_MESSAGE_CRAWLING)
} else {
log_event(glue("Subreddit '{current_subreddit}' has {round0(N_all_inactive)} inactive thread(s) of which {N_all_inactive_uncrawled} ({round1(0)}%) are uncrawled..."), message = PRINT_LOG_MESSAGE_CRAWLING)
}
# Start by crawling the ones we've never crawled before.
for (current_thread_id in uncrawled_thread_ids) {
# Make sure this thread is not blacklisted (for various reasons).
if (!(current_thread_id %in% thread_id_blacklist)) {
# Attempt to crawl the comments for this uncrawled thread, process them, and save them.
tryCatch({
# Extract the thread URL for this thread ID.
current_url <- current_sr_urls %>% filter(thread_id == current_thread_id) %>% select(url) %>% pull()
waiting_period <- sample(MIN_COMMENT_CRAWLING_BREAK:MAX_COMMENT_CRAWLING_BREAK, 1)
# message(glue("uncrawled\t{current_subreddit}\t\t{current_thread_id}\t\t{waiting_period} seconds"))   # status message
# message(glue("uncrawled\t{current_subreddit}\t\t{current_thread_id}\t\t{waiting_period} seconds\t{current_url}"))   # DEBUG
# Get the most recent comments from this subreddit and convert to a tibble.
log_event(glue("Requesting comments for uncrawled thread '{current_thread_id}' in '{current_subreddit}' ({waiting_period}s wait)..."), message = PRINT_LOG_MESSAGE_CRAWLING)
current_sr_thread <- get_thread_content(urls = current_url)   # Needs to be enclosed with try() statement
current_sr_thread_metadata <- current_sr_thread$threads %>% as_tibble()
current_sr_comments <- current_sr_thread$comments %>% as_tibble()
# Clean up the crawled thread metadata and comments. Make sure they are in
# the same format and merge them.
cleaned_thread_metadata <- current_sr_thread_metadata %>%
# Select the relevant columns.
select(author, timestamp, title, text, score, upvotes, downvotes, golds) %>%
# Add a readable timestamp column.
mutate(timestamp = as_datetime(timestamp)) %>%
# Clean the comment text.
mutate(comment = glue("{clean_text(title)} {clean_text(text)}")) %>%
# Add the subreddit and thread ID.
mutate(subreddit = current_subreddit,
thread_id = current_thread_id,
comment_id = "0") %>%
# Retain the relevant comments.
select(subreddit, thread_id, comment_id, author, timestamp, comment,
score, upvotes, downvotes, golds)
# Clean up the crawled comments. First check whether this thread has
# received comments already.
if (nrow(current_sr_comments) > 0) {
cleaned_comments <- current_sr_comments %>%
# Select the relevant columns.
select(comment_id, author, timestamp, comment, score, upvotes, downvotes, golds) %>%
# Add a readable timestamp column.
mutate(timestamp = as_datetime(timestamp)) %>%
# Clean the comment text.
mutate(comment = clean_text(comment)) %>%
# Add the subreddit and thread ID.
mutate(subreddit = current_subreddit,
thread_id = current_thread_id) %>%
relocate(subreddit, thread_id, .before = "comment_id")
# No comments yet? Then make an empty tibble.
} else {
cleaned_comments <- tibble(subreddit = character(), thread_id = character(),
comment_id = character(), author = character(),
timestamp = as_datetime(0), comment = character(),
score = numeric(), upvotes = numeric(),
downvotes = numeric(), golds = numeric())
}
# Merge the thread metadata and comments.
cleaned_comments <- rbind(cleaned_thread_metadata, cleaned_comments) %>%
# Sort in chronological order.
arrange(timestamp)
# Read in the comments currently crawled from this thread.
old_comments <- get_thread_comments(thread_crawl_dir, current_thread_id, prefix_length = PREFIX_LENGTH)
# Merge the newly crawled comments with the old crawl.
all_comments <- rbind(cleaned_comments, old_comments) %>%
# Remove the double quotes that for some reason keep getting inserted.
mutate(comment = clean_text(comment)) %>%
# Remove duplicate rows.
distinct() %>%
# Sort in reverse chronological order.
arrange(timestamp)
# Save the updated version to file.
log_event(glue("Saving new comments for uncrawled thread '{current_thread_id}' in '{current_subreddit}' to disk..."), message = PRINT_LOG_MESSAGE)
# comments_crawl_filename <- glue("{thread_crawl_dir}/comments.{current_thread_id}.tsv")    ### DELETE
# write_tsv(all_comments, comments_crawl_filename)                                            ### DELETE
save_thread_comments(all_comments, thread_crawl_dir, current_thread_id, prefix_length = PREFIX_LENGTH)
# Log this crawling event in the event log.
log_event(glue("Logging crawl of new comments for uncrawled thread '{current_thread_id}' in '{current_subreddit}'..."), message = PRINT_LOG_MESSAGE)
Sys.sleep(waiting_period)  # Wait a random period before going on the next one.
# Add this thread's ID to the list of successfully crawled thread IDs.
successfully_crawled_threads_list <- append(successfully_crawled_threads_list, current_thread_id)
# If anything goes wrong, catch the error and log it.
}, error = function(msg) {
log_event(glue("Error crawling new comments for uncrawled thread '{current_thread_id}' in '{current_subreddit}'..."), message = PRINT_LOG_MESSAGE)
}) # Close the tryCatch statement
}
} # Closes uncrawled thread loop
} # Closes subreddit loop
# Update the crawling log. All threads we crawled successfully, should be
# updated to 'done. Read in the current version, change the status of
# the relevant threads, then save again.
if (length(successfully_crawled_threads_list) > 0) {
log_event(glue("Updating crawling log threads in '{current_subreddit}' that are done crawling..."), message = PRINT_LOG_MESSAGE)
crawling_log %>%
# Update status to 'done'.
mutate(status = ifelse(thread_id %in% successfully_crawled_threads_list, "done", status)) %>%
# Update status to 'done' if we somehow missed this previously
mutate(status = ifelse(thread_id %in% inactive_crawled_thread_ids, "done", status)) %>%
# Save to file.
write_tsv(crawling_log_filename)
}
# crawling_log %>%
#   mutate(status = ifelse(thread_id %in% crawled_thread_ids, "done", status)) %>%
#   write_tsv(crawling_log_filename)
} # Closes comment crawling IF
